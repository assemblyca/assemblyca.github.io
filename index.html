<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>AssemblyCA| A Benchmark of Open-Endedness for Discrete Cellular Automata</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning",
            "assemblyca": "assemblyca",
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">AssemblyCA: A Benchmark of Open-Endedness for Discrete Cellular Automata</h1>
                    <h3 class="title is-4 conference-authors"><a target="_blank" href="https://icml.cc/">NeurIPS Workshop on Agent Learning in Open-Endedness (ALOE) 2023</a>
                    </h3>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a target="_blank" href="https://keithpatarroyo.netlify.app/">Keith&#160;Y.&#160;Patarroyo</a><sup>1</sup>,
                <a target="_blank" href="https://sites.google.com/site/aerospike2x/home">Abhishek&#160;Sharma</a><sup>1</sup>,
                <a target="_blank"
                   href="http://emergence.asu.edu/team.html">Sara&#160;I.&#160;Walker</a><sup>2</sup>,
                <a target="_blank" href="https://www.chem.gla.ac.uk/cronin/members/lee-cronin/">Leroy&#160;Cronin</a><sup>1</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Complex Chemistry Labs, University of Glasgow; </span>
                        <span class="author-block"><sup>2</sup>Arizona State University </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->

                            <span class="link-block">
                <a target="_blank" href="assets/vima_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
              </span>
                            <!-- TODO Code Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://github.com/vimalabs/VIMA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
            </span>
                <!-- TODO Dataset Link. -->
                <span class="link-block">
                <a target="_blank" href="https://huggingface.co/datasets/VIMA/VIMA-Data"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-light is-small">
    <div class="hero-body">
        <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-random_soup">
                    <video poster="" id="random_soup" autoplay controls muted loop height="100%"
                           playbackRate=2.0>
                        <source src="assets/videos/assemblyca/randomsoup/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-random_soup">
                    <video poster="" id="random_soup" autoplay controls muted loop height="100%"
                           playbackRate=2.0>
                        <source src="assets/videos/assemblyca/randomsoup/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                <div class="item item-random_soup">
                    <video poster="" id="random_soup" autoplay controls muted loop height="100%">
                        <source src="assets/videos/assemblyca/randomsoup/1.mp4"
                                type="video/mp4">
                    </video>
                </div>
                
            </div>
        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
                        We introduce AssemblyCA, a framework for utilizing cellular automata(CA) designed to benchmark the potential of open-ended processes. The benchmark quantifies the open-endedness of a system composed of resources, agents interacting with CAs, and a set of generated artifacts. We quantify the amount of open-endedness by taking the generated artifacts or objects and analyzing them using the tools of assembly theory(AT). Assembly theory can be used to identify selection in systems that produce objects that can be decomposable into atomic units, where these objects can exist in high copy numbers. By combining an assembly space measure with the copy number of an object we can quantify the complexity of objects that have a historical contingency. Moreover, this framework allows us to accurately quantify the indefinite generation of novel, diverse, and complex objects, the signature of open-endedness. We benchmark different measures from the assembly space with standard diversity and complexity measures that lack historical contingency. Finally, the open-endedness of three different systems is quantified by performing an undirected exploration in two-dimensional life-like CA, a cultural exploration provided by human experimenters, and an algorithmic exploration by a set of programmed agents.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <img src="assets/images/Picture1.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%"><b>Multimodal prompts for task specification.</b> We observe that many robot manipulation tasks can be expressed as <i>multimodal prompts</i> that interleave language and image/video frames. We propose VIMA, an embodied agent capable of processing mulitimodal prompts (left) and controlling a robot arm to solve the task (right).</span>
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Assembly Theory</span></h2>
                    <img src="assets/images/Picture2.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
<span style="font-weight: bold">VIMA architecture.</span> We encode the multimodal prompts with a pre-trained T5 model, and condition the robot controller on the prompt through cross-attention layers. The controller is a causal transformer decoder consisting of alternating self and cross attention layers that predicts motor commands conditioned on prompts and interaction history.</span>
                <span style="font-size: 110%">
                        <span style="font-weight: bold">Ablation on visual tokenizers.</span> We compare the performance of VIMA-200M model across different visual tokenizers. Our proposed object tokens outperform all methods that learn directly from raw pixels, and <i>Object Perceiver</i> that downsamples the object sequence to a fixed number of tokens.
                    </span>

                    <br>
                    <br>
                    <br>
                    <br>

                    <img src="assets/images/Picture3.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Ablation on prompt conditioning.</span> We compare our method (<i>xattn</i>: cross-attention prompt conditioning) with a vanilla transformer decoder (<i>gpt-decoder</i>) across different model sizes. Cross-attention is especially helpful in low-parameter regime and for harder generalization tasks.
                    </span>
                </div>
            </div>

        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">

        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Benchmarks</span></h2>
                    <span style="font-size: 125%">
                        We provide 17 representative tasks with multimodal prompt templates, which can be procedurally instantiated into thousands of individual instances by various combinations of textures and tabletop objects.
                    </span>

                    <br>
                    <br>
                    <br>
                    <img src="assets/images/Picture4.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    </div>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Experiments</span></h2>

                    <p style="font-size: 125%">
                        We perform three experiments to showcase our benchmark:
                    <ul style="font-size: 125%; padding-left: 5%">
                        <li>
                            1.  Undirected process
                        </li>
                        <li>
                            2. Open-ended process that is undergoing selection
                        </li>
                        <li>
                            3. Algorithmic process with programmed agents
                        </li>

                    </ul>
                    </p>
                    <br>
                    <br>

                    <h3 class="title is-4"><span
                            class="dvima">Undirected Process</span></h3>

                    <img src="assets/images/Picture6a.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Scaling model and data.</span> <i>Top</i>: We compare performance of different methods with model sizes ranging from 2M to 200M parameters. Across all model sizes and generalization levels, VIMA outperforms baseline variants. <i>Bottom</i>: For a fixed model size of 92M parameters we compare the effect of imitation learning dataset size with 0.1%, 1%, 10%, and full data. VIMA is extremely sample efficient and can achieve performance comparable to other methods with 10x less data.
                    </span>
                    <br>
                    <br>
                    <br>

                    <h3 class="title is-4"><span
                            class="dvima">Open Ended Process </span></h3>

                    <br>

                    <img src="assets/images/Picture6b.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Ablation on visual tokenizers.</span> We compare the performance of VIMA-200M model across different visual tokenizers. Our proposed object tokens outperform all methods that learn directly from raw pixels, and <i>Object Perceiver</i> that downsamples the object sequence to a fixed number of tokens.
                    </span>

                    <br>
                    <br>
                    <br>
                    <br>

                    <h3 class="title is-4"><span
                            class="dvima">Algorithmic Agent</span></h3>

                    <img src="assets/images/Picture7.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
                        <span style="font-weight: bold">Scaling model and data.</span> <i>Top</i>: We compare performance of different methods with model sizes ranging from 2M to 200M parameters. Across all model sizes and generalization levels, VIMA outperforms baseline variants. <i>Bottom</i>: For a fixed model size of 92M parameters we compare the effect of imitation learning dataset size with 0.1%, 1%, 10%, and full data. VIMA is extremely sample efficient and can achieve performance comparable to other methods with 10x less data.
                    </span>
                    <br>
                    <br>
                    <br>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Conclusion-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Conclusion</span></h2>

                    <p style="font-size: 125%">
                        In this work, we introduce a novel <i>multimodal</i> prompting formulation that converts diverse
                        robot manipulation tasks into a uniform sequence modeling problem. We instantiate this
                        formulation in VIMA-Bench, a diverse benchmark with multimodal tasks and systematic evaluation
                        protocols for generalization. We propose VIMA, a conceptually simple transformer-based agent
                        capable of solving tasks such as visual goal reaching, one-shot video imitation, and novel
                        concept grounding with a single model. Through comprehensive experiments, we show that VIMA
                        exhibits strong model scalability and zero-shot generalization. Therefore, we recommend our
                        agent design as a solid starting point for future work.
                    </p>

                </div>
            </div>

        </div>
    </div>
</section>


<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{jiang2023vima,
  title     = {VIMA: General Robot Manipulation with Multimodal Prompts},
  author    = {Yunfan Jiang and Agrim Gupta and Zichen Zhang and Guanzhi Wang and Yongqiang Dou and Yanjun Chen and Li Fei-Fei and Anima Anandkumar and Yuke Zhu and Linxi Fan},
  booktitle = {Fortieth International Conference on Machine Learning},
  year      = {2023}
}</code></pre>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column">
                <div class="content has-text-centered">
                    <p>
                        Website template borrowed from <a
                            href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                            href="https://github.com/cliport/cliport.github.io">CLIPort</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
